{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With currency category i.e. concepts, entities, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3033, 10983)\n",
      "(3033, 1)\n",
      "[[  0.   0.   0. ...  81.  76. 817.]\n",
      " [  0.   0.   0. ...   0.   0. 803.]\n",
      " [  0.   0.   0. ...  77.  99. 820.]\n",
      " ...\n",
      " [  0.   0.   0. ...  91.  90. 815.]\n",
      " [  0.   0.   0. ...  65.  70. 812.]\n",
      " [  0.   0.   0. ...  51.  41. 814.]]\n"
     ]
    }
   ],
   "source": [
    "vocab_file='dataset.csv'\n",
    "label_file='label1.csv'\n",
    "df= pd.read_csv(vocab_file)\n",
    "df2=pd.read_csv(label_file)\n",
    "print(df.shape)\n",
    "print(df2.shape)\n",
    "print(df.values)\n",
    "#df2\n",
    "label_dict={'not spam': 1, 'spam': 0}\n",
    "df2['news-label']= df2['news-label'].map(label_dict)\n",
    "y=df2['news-label']\n",
    "#print(y)\n",
    "#x= df\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
      "0.7960526315789473\n",
      "0.8039538714991763\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_sum=0\n",
    "\n",
    "precision_micro_sum=0\n",
    "precision_macro_sum=0\n",
    "precision_weighted_sum=0\n",
    "\n",
    "recall_micro_sum=0\n",
    "recall_macro_sum=0\n",
    "recall_weighted_sum=0\n",
    "\n",
    "f1_micro_sum=0\n",
    "f1_macro_sum=0\n",
    "f1_weighted_sum=0\n",
    "\n",
    "clf= ensemble.GradientBoostingClassifier()\n",
    "skf=StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(df, y)\n",
    "print(skf)\n",
    "for train_index, test_index in skf.split(df, y):\n",
    "    x_train, x_test= df.iloc[train_index], df.iloc[test_index]\n",
    "    y_train, y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "#Accuracy calculation    \n",
    "    accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "    accuracy_sum=accuracy_sum+accuracy    \n",
    "#Precision calculation\n",
    "    precision_micro=metrics.precision_score(y_test,y_pred, average='micro')\n",
    "    print(precision_micro)\n",
    "    precision_micro_sum=precision_micro_sum+precision_micro\n",
    "    precision_macro=metrics.precision_score(y_test,y_pred, average='macro')\n",
    "    precision_macro_sum=precision_macro_sum+precision_macro\n",
    "    precision_weighted=metrics.precision_score(y_test,y_pred, average='weighted')\n",
    "    precision_weighted_sum=precision_weighted_sum+precision_weighted\n",
    "#recall calculation    \n",
    "    recall_micro= metrics.recall_score(y_test, y_pred, average='micro')\n",
    "    recall_micro_sum= recall_micro_sum+recall_micro\n",
    "    recall_macro= metrics.recall_score(y_test, y_pred, average='macro')\n",
    "    recall_macro_sum= recall_macro_sum+recall_macro    \n",
    "    recall_weighted= metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "    recall_weighted_sum= recall_weighted_sum+recall_weighted    \n",
    "#f1 calculation\n",
    "    f1_micro=f1_score(y_test, y_pred, average='micro')\n",
    "    f1_micro_sum=f1_micro_sum+f1_micro\n",
    "    f1_macro=f1_score(y_test, y_pred, average='macro')\n",
    "    f1_macro_sum=f1_macro_sum+f1_macro\n",
    "    f1_weighted=f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_weighted_sum=f1_weighted_sum+f1_weighted\n",
    "#ROC curve    \n",
    "    y_pred= clf.predict_proba(x_test)\n",
    "    skplt.metrics.plot_roc(y_test, y_pred)\n",
    "\n",
    "        \n",
    "    \n",
    "#accuracy average    \n",
    "accuracy_average= accuracy_sum/5\n",
    "#precision average\n",
    "precision_micro_average=precision_micro_sum/5\n",
    "precision_macro_average=precision_macro_sum/5\n",
    "precision_weighted_average=precision_weighted_sum/5\n",
    "#recall average\n",
    "recall_micro_average=recall_micro_sum/5\n",
    "recall_macro_average=recall_macro_sum/5\n",
    "recall_weighted_average=recall_weighted_sum/5\n",
    "#f1 average\n",
    "f1_micro_average=f1_micro_sum/5\n",
    "f1_macro_average=f1_macro_sum/5\n",
    "f1_weighted_average=f1_weighted_sum/5\n",
    "\n",
    "print('Gradient Boosting Classifier: ')\n",
    "print('Accuracy score is :                ',accuracy_average*100)\n",
    "print(\"\")\n",
    "print('Micro precision score is:          ',precision_micro_average*100)\n",
    "print('Macro precision score is:          ',precision_macro_average*100)\n",
    "print('Weighted precision score is:       ',precision_weighted_average*100)\n",
    "print(\"\")\n",
    "print('Micro Recall score is:             ',recall_micro_average*100)\n",
    "print('Macro Recall score is:             ',recall_macro_average*100)\n",
    "print('Weighted Recall score is:          ',recall_weighted_average*100)\n",
    "print(\"\")\n",
    "print('Micro f1 score is:                 ', f1_micro_average*100)\n",
    "print('Macro f1 score is:                 ', f1_macro_average*100)\n",
    "print('Weighted f1 score is:              ', f1_weighted_average*100)\n",
    "#accuracy_average= accuracy_sum/5    \n",
    "#print(accuracy_average*100)  \n",
    "#df.iloc[train_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_sum=0\n",
    "\n",
    "precision_micro_sum=0\n",
    "precision_macro_sum=0\n",
    "precision_weighted_sum=0\n",
    "\n",
    "recall_micro_sum=0\n",
    "recall_macro_sum=0\n",
    "recall_weighted_sum=0\n",
    "\n",
    "f1_micro_sum=0\n",
    "f1_macro_sum=0\n",
    "f1_weighted_sum=0\n",
    "\n",
    "clf= DecisionTreeClassifier(random_state=0)\n",
    "skf=StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(df, y)\n",
    "print(skf)\n",
    "for train_index, test_index in skf.split(df, y):\n",
    "    x_train, x_test= df.iloc[train_index], df.iloc[test_index]\n",
    "    y_train, y_test= y.iloc[train_index], y.iloc[test_index]\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred=clf.predict(x_test)\n",
    "#Accuracy calculation    \n",
    "    accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "    accuracy_sum=accuracy_sum+accuracy    \n",
    "#Precision calculation\n",
    "    precision_micro=metrics.precision_score(y_test,y_pred, average='micro')\n",
    "    print(precision_micro)\n",
    "    precision_micro_sum=precision_micro_sum+precision_micro\n",
    "    precision_macro=metrics.precision_score(y_test,y_pred, average='macro')\n",
    "    precision_macro_sum=precision_macro_sum+precision_macro\n",
    "    precision_weighted=metrics.precision_score(y_test,y_pred, average='weighted')\n",
    "    precision_weighted_sum=precision_weighted_sum+precision_weighted\n",
    "#recall calculation    \n",
    "    recall_micro= metrics.recall_score(y_test, y_pred, average='micro')\n",
    "    recall_micro_sum= recall_micro_sum+recall_micro\n",
    "    recall_macro= metrics.recall_score(y_test, y_pred, average='macro')\n",
    "    recall_macro_sum= recall_macro_sum+recall_macro    \n",
    "    recall_weighted= metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "    recall_weighted_sum= recall_weighted_sum+recall_weighted    \n",
    "#f1 calculation\n",
    "    f1_micro=f1_score(y_test, y_pred, average='micro')\n",
    "    f1_micro_sum=f1_micro_sum+f1_micro\n",
    "    f1_macro=f1_score(y_test, y_pred, average='macro')\n",
    "    f1_macro_sum=f1_macro_sum+f1_macro\n",
    "    f1_weighted=f1_score(y_test, y_pred, average='weighted')\n",
    "    f1_weighted_sum=f1_weighted_sum+f1_weighted\n",
    "#ROC curve    \n",
    "    y_pred= clf.predict_proba(x_test)\n",
    "    skplt.metrics.plot_roc(y_test, y_pred)\n",
    "\n",
    "        \n",
    "    \n",
    "#accuracy average    \n",
    "accuracy_average= accuracy_sum/5\n",
    "#precision average\n",
    "precision_micro_average=precision_micro_sum/5\n",
    "precision_macro_average=precision_macro_sum/5\n",
    "precision_weighted_average=precision_weighted_sum/5\n",
    "#recall average\n",
    "recall_micro_average=recall_micro_sum/5\n",
    "recall_macro_average=recall_macro_sum/5\n",
    "recall_weighted_average=recall_weighted_sum/5\n",
    "#f1 average\n",
    "f1_micro_average=f1_micro_sum/5\n",
    "f1_macro_average=f1_macro_sum/5\n",
    "f1_weighted_average=f1_weighted_sum/5\n",
    "\n",
    "print('Decision Tree Classifier: ')\n",
    "print('Accuracy score is :                ',accuracy_average*100)\n",
    "print(\"\")\n",
    "print('Micro precision score is:          ',precision_micro_average*100)\n",
    "print('Macro precision score is:          ',precision_macro_average*100)\n",
    "print('Weighted precision score is:       ',precision_weighted_average*100)\n",
    "print(\"\")\n",
    "print('Micro Recall score is:             ',recall_micro_average*100)\n",
    "print('Macro Recall score is:             ',recall_macro_average*100)\n",
    "print('Weighted Recall score is:          ',recall_weighted_average*100)\n",
    "print(\"\")\n",
    "print('Micro f1 score is:                 ', f1_micro_average*100)\n",
    "print('Macro f1 score is:                 ', f1_macro_average*100)\n",
    "print('Weighted f1 score is:              ', f1_weighted_average*100)\n",
    "#accuracy_average= accuracy_sum/5    \n",
    "#print(accuracy_average*100)  \n",
    "#df.iloc[train_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
